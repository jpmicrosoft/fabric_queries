{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# External Access Audit \u2014 PySpark Notebook\n",
        "\n",
        "This notebook filters **activityEventEntities** in a Lakehouse to find events indicating **external data access**, then writes summary tables for Power BI.\n",
        "\n",
        "**External-access operations included:**\n",
        "- `AcceptExternalDataShare`\n",
        "- `AddExternalResource`\n",
        "- `AddLinkToExternalResource`\n",
        "- `AnalyzedByExternalApplication`\n",
        "\n",
        "> Source: Microsoft Fabric Operation list (Audit) and Activity Events API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# --- Configuration: edit these for your environment ---\n",
        "lakehouse_path = \"/lakehouse/audit/activityEventEntities\"  # Delta path containing exported activity events\n",
        "external_ops = [\n",
        "    \"AcceptExternalDataShare\",\n",
        "    \"AddExternalResource\",\n",
        "    \"AddLinkToExternalResource\",\n",
        "    \"AnalyzedByExternalApplication\"\n",
        "]\n",
        "\n",
        "# Date range example (UTC).\n",
        "start_date = \"2025-11-01\"\n",
        "end_date   = \"2025-12-02\"\n",
        "\n",
        "# Output locations (Delta)\n",
        "actor_summary_path = \"/lakehouse/audit/summary_actor\"\n",
        "operation_summary_path = \"/lakehouse/audit/summary_operation\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# --- PySpark analysis ---\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ExternalAccessAuditAnalysis\").getOrCreate()\n",
        "\n",
        "# Load activity events from Delta\n",
        "df = spark.read.format(\"delta\").load(lakehouse_path)\n",
        "\n",
        "# Filter by operations and date range\n",
        "filtered_df = df.filter(\n",
        "    (col(\"Operation\").isin(external_ops)) &\n",
        "    (col(\"Timestamp\") >= start_date) &\n",
        "    (col(\"Timestamp\") <= end_date)\n",
        ")\n",
        "\n",
        "display_cols = [\"Timestamp\", \"Actor\", \"Operation\", \"WorkspaceId\", \"ItemId\"]\n",
        "filtered_df.select(*display_cols).show(50, truncate=False)\n",
        "\n",
        "# Summaries\n",
        "summary_actor = filtered_df.groupBy(\"Actor\").agg(count(\"Operation\").alias(\"ExternalAccessCount\"))\n",
        "summary_operation = filtered_df.groupBy(\"Operation\").agg(count(\"Actor\").alias(\"AccessCount\"))\n",
        "\n",
        "summary_actor.show()\n",
        "summary_operation.show()\n",
        "\n",
        "# Persist summaries to Delta for Power BI\n",
        "summary_actor.write.format(\"delta\").mode(\"overwrite\").save(actor_summary_path)\n",
        "summary_operation.write.format(\"delta\").mode(\"overwrite\").save(operation_summary_path)\n",
        "\n",
        "# (Optional) Register as tables for SQL access in Fabric\n",
        "spark.sql(f\"DROP TABLE IF EXISTS ActorSummary\")\n",
        "spark.sql(f\"DROP TABLE IF EXISTS OperationSummary\")\n",
        "spark.sql(f\"CREATE TABLE ActorSummary USING DELTA LOCATION '{actor_summary_path}'\")\n",
        "spark.sql(f\"CREATE TABLE OperationSummary USING DELTA LOCATION '{operation_summary_path}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notes\n",
        "- The `Timestamp` column is expected to be in ISO-8601 (UTC).\n",
        "- If your lakehouse path differs, update `lakehouse_path`.\n",
        "- In a non-Delta environment, load as CSV/Parquet and adjust `read.format(...)`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}